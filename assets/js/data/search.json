[ { "title": "[Docker] Docker 이미지 최적화(with Node.js) - (1/3)", "url": "/posts/optimizing-base-image(node.js)/", "categories": "Infra, Docker", "tags": "Docker", "date": "2023-08-17 00:00:00 +0900", "snippet": "목차 Default node image Docker Hub options Slimer images Alpine images Distroless images Conclusion Reference이미지 크기와 취약성은 CI/CD 파이프라인 및 보안 태세에 큰 영향을 미칠 수 있기에 Base 이미지를 선택하는 것은 최적화를 위해 필요하다.Node.js 이미지 빌드 시 사용할 수 있는 옵션으로는 Core Node.js 팀에서 유지 관리하는 공식 Node.js 이미지, Docker Hub repository에서 제공하는 tag가 달린 이미지, Google에서 관리하는 distroless 이미지 등이 존재한다.1 Default node image nodeNode.js Docker 팀에서 공식적으로 관리하며 여러 Docker Base 이미지 태그가 포함되어 있다. 해당 태그는 다양한 OS 버전(Debian, Ubuntu, Alpine 등)과 Node.js 런타임 자체에 매핑되는 여러 Docker Base 이미지 태그를 포함한다. 또한, amd64 또는 arm64x8(Apple M1)과 같은 CPU 아키텍처를 대상으로 하는 특정 버전 태그도 존재한다FROM node위 이미지를 빌드하게 되면, node:latest 버전으로 설치가 진행되며, 이미지 크기는 1GB 이다. 해당 이미지에서 발생할 수 있는 종속성 및 보안 취약성은 다음과 같다. 총 409개의 종속성: curl/libcurl4, git/git-man 또는 imagemagick/imagemagick-6-common과 같은 운영 체제 패키지 관리자를 사용하였을 때 409개의 오픈 소스 라이브러리가 감지되었음 총 289개의 보안 문제: 위 종속성 내에서 Buffer Overflows, Use After Free errors, Out-of-bounds Write 등 총 289개의 보안 문제가 발견되었음 Node.js 18.2.0 런타임 버전은 DNS Rebinding, HTTP Request Smuggling, Configuration Hijacking 등 7 가지 보안 문제에 취약함 또한, 해당 이미지 내에는 wget, git, curl과 같은 유틸리티가 default 로 설치되어 있다2 Docker Hub options node:buster vs node:bullseye node:buster, node:bullseyeNode.js Docker Hub 레포지토리에서 사용 가능한 태그를 찾아보면 node:buster , node:bullseye를 찾을 수 있다. 두 태그는 모두 Debian 배포 버전을 기반으로 하고 있으며, buster 태그는 24년에 종료되는 Debian 10에 매핑되므로 좋은 선택은 아니다. bullseye 태그는 26년 6월 경 종료되는 Debian 11(Current stable release)에 매핑되므로 bullseye 태그를 사용하는 것이 보다 더 적절하다(Debian LTS)FROM node:bullseye하지만, 위 방식으로 이미지를 빌드하면 node 이미지와 완전하게 동일한 종속성 및 보안 취약성 결과가 나타난다. 이는 node, node:buster, node:bullseye 모두 동일한 Node.js 이미지 태그를 가리키기 때문이다.3 Image tag for slimmer images node:bullseye-slimslim이 포함된 이미지는 기본 태그에 포함된 공통 패키지를 포함하지 않으며 node를 실행하기 위한 최소한의 패키지만 포함한다.FROM node:bullseye-slim위 이미지 크기는 약 245MB 이며, 콘텐츠를 스캔하면 97개의 종속성과 56개의 취약점이 나타난다. 기본 node 이미지와 비교해 보았을 때 이미지 크기 및 보안 태세 측면에서 더 나은 것을 확인할 수 있다.4 Image tag for alpine images node:alpinealpine(Alpine Linux) 버전은 slim 버전 보다 더 작은 약 180MB 이하의 이미지 크기를 갖는다. 이는 소프트웨어 설치 공간이 더 작다는 것을 의미하고 이는 취약성으로 나타나는 부분이 더 적다는 것을 의미한다.FROM node:alpine위 이미지의 콘텐츠를 스캔하였을 때, 총 16개의 운영 체제 종속성과 2개의 보안 취약점이 감지 되었으며 이는 slim 보다 이미지 크기 및 보안 태세 측면 모두에서 더 좋은 선택 임을 알 수 있다.그렇다면 alpine 이 가장 최선의 선택인 걸까?이미지 크기 및 보안 태세 측면에서는 좋은 선택일 수 있다. 하지만, Alpine 프로젝트는 musl을 C 표준 라이브러리의 구현으로 사용하는 반면 bullseye 또는 slim 과 같은 Debian의 Node.js 이미지 태그는 glibc 구현에 의존한다.이러한 차이는 기본 C 라이브러리 차이로 인한 성능 문제, 기능적 버그 또는 잠재적인 응용 프로그램 충돌을 야기할 수도 있다. 뿐만 아니라, Node.js Docker 팀은 Alpine을 기반으로 하는 컨테이너 이미지 빌드를 공식적으로 지원하지 않는다. 따라서, Alpine 기반 이미지 태그는 실험적이고 일관성이 부족할 수도 있게 때문에 운영(Production) 단계에서 사용하기에 적합하지 않다. Node.js Unofficial Builds Project Unofficial-builds attempts to provide basic Node.js binaries for some platforms that are either not supported or only partially supported by Node.js. This project does not provide any guarantees and its results are not rigorously tested. Builds made available at Node.js have very high quality standards for code quality, support on the relevant platforms platforms and for timing and methods of delivery. Builds made available by unofficial-builds have minimal or no testing; the platforms may have no inclusion in the official Node.js test infrastructure. These builds are made available for the convenience of their user community but those communities are expected to assist in their maintenance.5 Distroless Docker Images for Node.js gcr.io/distroless/nodejs:16distroless 이미지는 애플리케이션과 해당 런타임 종속성만 대상으로 하기 때문에 slim 이미지 태그 보다 더 적은 용량을 차지한다. 즉, 컨테이너 패키지 매니저, 쉘 그리고 기타 범용 도구 종속성이 없으므로 작은 크기와 취약성을 띠고 있다.FROM gcr.io/distroless/nodejs:16Distroless 컨테이너 이미지에는 소프트웨어가 없기 때문에 Docker에서 Multi-stage 워크 플로우를 사용하여 컨테이너에 대한 종속성을 설치하고 이를 distroless 이미지에 복사하여 사용한다.FROM node:16-bullseye-slim AS buildWORKDIR /usr/src/appCOPY . /usr/src/appRUN npm installFROM gcr.io/distroless/nodejs:16COPY --from=build /usr/src/app /usr/src/appWORKDIR /usr/src/appCMD [\"server.js\"]distroless 이미지는 Debian 기반으로 되어 있으며, 안정적인 릴리즈(current stable release) 버전을 이미지로 제공한다. 하지만, 세분화된 Node.js 런타임 버전을 제공하지 않고 자주 업데이트되는 범용 nodejs:16 태그를 사용하거나 특정 시점의 SHA256 해시를 기반으로 설치해야 한다.6 Conclusion위 다양한 경우를 토대로 보았을 때, 가장 이상적인 Node.js Docker 이미지는 안정적(stable)이고 장기 지원 버전(active Long Term Support Version)이 있어야 하고, 최신 Debian OS를 기반으로 하는 slim 버전이다.FROM node:16-bullseye-slimReference https://snyk.io/blog/choosing-the-best-node-js-docker-image/ https://thearchivelog.dev/article/optimize-docker-image/ https://github.com/vercel/next.js/discussions/16995 https://blog.alexellis.io/mutli-stage-docker-builds/" }, { "title": "[Docker] Docker Multi-stage builds - (2/3)", "url": "/posts/multi-stage-builds/", "categories": "Infra, Docker", "tags": "Docker", "date": "2023-08-17 00:00:00 +0900", "snippet": "목차 Before multi-stage Builder pattern Multi-stage builds Multi-stage build example Multi-statge 유용한 기능들 빌드 단계 이름 지정 특정 빌드 단계에서 중지 외부 이미지를 스테이지로 사용 이전 스테이지를 새 스테이지로 사용 응용 예제 Buildkit 활용 Reference1. Before multi-stage builds(Builder pattern)Multi-stage builds 가 등장하기 전에는 Dockerfile 이미지의 크기를 줄이기 위하여 Dockerfile을 개발용으로 사용하고 다른 하나는 프로덕션 용으로 사용하는 것이 일반적 이었다. 이처럼 두 개의 Docker 이미지를 사용하여 하나는 빌드를 수행하고 다른 하나는 첫 번째 이미지에서 필요한 부분만 추출하여 재빌드 하는 것을 Builder pattern 이라고 한다.Builder pattern Dockerfile.build: 개발 및 빌드용, 애플리케이션을 구축하는 데 필요한 모든 것이 포함 Dockerfile: 간소화된 프로덕션 빌드용, 애플리케이션과 이를 실행하는 데 필요한 종속 항목만 포함1.2 Builder pattern example Dockerfile.build# syntax=docker/dockerfile:1FROM golang:1.16WORKDIR /go/src/github.com/alexellis/href-counter/COPY app.go ./RUN go get -d -v golang.org/x/net/html \\ &amp;&amp; CGO_ENABLED=0 go build -a -installsuffix cgo -o app . Dockerfile# syntax=docker/dockerfile:1FROM alpine:latestRUN apk --no-cache add ca-certificatesWORKDIR /root/COPY app ./CMD [\"./app\"] build.sh Dockerfile.build 를 빌드한다. 아티팩트를 복사하기 위한 컨테이너를 생성한다 간소화된 프로덕션용 이미지를 빌드한다. #!/bin/shecho Building alexellis2/href-counter:builddocker build -t alexellis2/href-counter:build . -f Dockerfile.build#아티팩트를 복사하기 위한 컨테이너 생성docker container create --name extract alexellis2/href-counter:builddocker container cp extract:/go/src/github.com/alexellis/href-counter/app ./appdocker container rm -f extractecho Building alexellis2/href-counter:latestdocker build --no-cache -t alexellis2/href-counter:latest .rm ./app builder pattern 를 통해 프로덕션용 이미지를 만들기 위해서는 두 개의 Docker 이미지가 시스템에서 공간을 차지하며 로컬 디스크에도 아티팩트가 남게된다.2. Multi-stage buildsmulti-stage build는 별도의 파일을 분리하여 관리하는 번거로움 없이 builder pattern의 이점을 제공한다.muti-stage builds 다단계 빌드에서는 Dockerfile 내에서 FROM 구문을 여러번 추가해 사용하는 것으로, 마지막에 선언한 FROM 문이 최종 Base image로 사용함 이전 이미지에서 생성된 아티팩트를 복사히기 위해서는 COPY --from=&lt;base_image_number&gt; 를 사용하면 됨 multi-stage builds 방식은 중간 이미지를 만들 필요가 없으며 로컬 시스템에 아티팩트를 추출할 필요 또한 없다.2.1 Multi-stage builds example Dockerfile# syntax=docker/dockerfile:1FROM golang:1.16WORKDIR /go/src/github.com/alexellis/href-counter/RUN go get -d -v golang.org/x/net/htmlCOPY app.go ./RUN CGO_ENABLED=0 go build -a -installsuffix cgo -o app .FROM alpine:latestRUN apk --no-cache add ca-certificatesWORKDIR /root/COPY --from=0 /go/src/github.com/alexellis/href-counter/app ./CMD [\"./app\"] builder-pattern 과 달리 복잡성이 크게 감소한 것을 확인할 수 있다. 기본적으로 스테이지에는 이름이 지정되지 않으며 첫 번째 스테이지는 0부터 시작하여 정수로 스테이지를 참조한다. 즉, golang:1.16 이미지가 가장 처음 스테이지 이므로 해당 스테이지의 번호는 0 번이다. 따라서, alpine:latest 에 있는 COPY --from=0 은 이전 단계에서 빌드된 아티팩트(app)를 현재 단계로 복사 하겠다는 뜻으로 해석할 수 있다. COPY --from 구문에서 선언하지 않은 아티팩트는 현재 이미지에 저장되지 않는다. (Ex. Go SDK, intermediate artifacts)2.2 Multi-stage 유용한 기능들2.2.1 빌드 단계 이름 지정# syntax=docker/dockerfile:1FROM golang:1.16 AS builderWORKDIR /go/src/github.com/alexellis/href-counter/RUN go get -d -v golang.org/x/net/htmlCOPY app.go ./RUN CGO_ENABLED=0 go build -a -installsuffix cgo -o app .FROM alpine:latestRUN apk --no-cache add ca-certificatesWORKDIR /root/COPY --from=builder /go/src/github.com/alexellis/href-counter/app ./CMD [\"./app\"] 스테이지를 번호로 참조하는 대신, AS &lt;NAME&gt; 구문을 사용하여 각 스테이지를 이름으로 참조할 수 있다.2.2.2 특정 빌드 단계에서 중지docker build --target builder -t alexellis2/href-counter:latest . Dockerfile 빌드 시 -target &lt;STAGE NAME&gt; 옵션을 통해 특정 스테이지 단계까지만 빌드하는 것이 가능하다. 이는 다음과 같은 경우에 유용할 수 있다. 특정 빌드 단계 디버깅 debug 스테이지 이미지를 빌드하여, 디버깅이 필요한 곳에서 재사용 2.2.3 외부 이미지를 스테이지로 사용COPY --from=nginx:latest /etc/nginx/nginx.conf /nginx.conf COPY --from=&lt;base_image&gt;는 Dockerfile 내에 선언된 스테이지 뿐만 아니라, 로컬 이미지 또는 Docker 레지스트리에 보관되어 있는 이미지에서 복사 할 수 있다.2.2.4 이전 스테이지를 새 스테이지로 사용# syntax=docker/dockerfile:1FROM alpine:latest AS builderRUN apk --no-cache add build-baseFROM builder AS build1COPY source1.cpp source.cppRUN g++ -o /binary source.cppFROM builder AS build2COPY source2.cpp source.cppRUN g++ -o /binary source.cpp FROM 구문을 사용하여 새로운 스테이지를 생성할 때, 이전에 사용했던 스테이지를 기본 이미지로 선택할 수 있다. 특정 빌드 단계에서 중지하는 옵션과 함께 사용한다면, 개발 환경과 프로덕션 환경을 분리하여 빌드 프로세스를 구성할 수 있다.2.2.5 특정 빌드 단계 중지 + 이전 스테이지를 새 스테이지로 사용(ex. Next.js)# Build target base ######################FROM node:14-alpine AS baseWORKDIR /appARG NODE_ENV=productionENV PATH=/app/node_modules/.bin:$PATH \\ NODE_ENV=\"$NODE_ENV\"RUN apk --no-cache add curlCOPY package.json yarn.lock /app/EXPOSE 3000# Build target dependencies ##############################FROM base AS dependencies# Install prod dependenciesRUN yarn install --production &amp;&amp; \\ # Cache prod dependencies cp -R node_modules /prod_node_modules &amp;&amp; \\ # Install dev dependencies yarn install --production=false# Build target development #############################FROM dependencies AS developmentCOPY . /appCMD [ \"yarn\", \"dev\" ]# Build target builder #########################FROM base AS builderCOPY --from=dependencies /app/node_modules /app/node_modulesCOPY . /appRUN yarn build &amp;&amp; \\ rm -rf node_modules# Build target production ############################FROM base AS productionCOPY --from=builder /app/public /app/publicCOPY --from=builder /app/.next /app/.nextCOPY --from=dependencies /prod_node_modules /app/node_modulesCMD [ \"yarn\", \"start\" ] 개발 환경(development)은 별도의 빌드 과정을 필요로 하지 않기 때문에 Base 이미지(base)와 패키지를 설치(dependencies)하는 단계만 포함하면 된다. 따라서, 다음 명령어를 통해 개발 환경용 이미지를 빌드할 수 있다.docker build --target development -t bulidtest/development:latest . 운영 환경(production)은 Base 이미지(base)와 패키지 설치(dependencies) 그리고 빌드(builder) 과정 모두가 필요하다. 따라서, 다음과 같은 명령어를 통해 운영 환경동 이미지를 빌드할 수 있다.docker build --target production -t buildtest/production:latest .2.2.6 BuildKit 활용DOCKER_BUILDKIT=1 docker build . BuildKit 은 버전 23.0 부터 Docker Desktop 및 Docker Engine 사용자를 위한 기본 빌더이다. BuildKit은 다음과 같은 기능을 지원한다. 사용하지 않는 빌드 단계 실행 감지 및 건너뛰기 (Docker 레거시 버전은 BuildKit을 사용해야 --target 옵션 사용할 수 있다) 독립적인 빌드 단계 구축 병렬화 (스테이지가 상호 독립적인 경우 각 스테이지를 병렬로 빌드할 수 있다) Reference https://docs.docker.com/build/building/multi-stage/#stop-at-a-specific-build-stage https://blog.alexellis.io/mutli-stage-docker-builds/" }, { "title": "[Docker] Best practices for Dockerfile - (3/3)", "url": "/posts/best-practices-for-dockerfiles/", "categories": "Infra, Docker", "tags": "Docker, Dockerfile", "date": "2023-08-17 00:00:00 +0900", "snippet": "목차 Layer 최적화 Docker 이미지 저장 방식 레이어 수 줄이기 명령문(Instructions) 정렬 패키지 최소화 애플리케이션 패키지 최소화 OS 패키지 최소화 Reference1. Layer 수 최소화1.1 Docker 이미지가 저장되는 방식Docker 이미지를 빌드하거나 다른 레포지토리로 부터 pull하는 경우 여러 hash 형태의 문자열이 화면에 보이는 것을 확인할 수 있다.$ docker pull ubuntu:15.04Using default tag: latestlatest: Pulling from library/ubuntuc499e6d256d6: Already exists74cda408e262: Pull completeffadbd415ab7: Pull completeDigest: sha256:282530fcb7cd19f3848c7b611043f82ae4be3781cb00105a1d593d7e6286b596Status: Downloaded newer image for ubuntu:15.04docker.io/library/ubuntu:15.04이렇게 분리된 데이터를 레이어(Layer) 라고 한다. 레이어는 Docker 이미지를 빌드할 때 Dockerfile에 정의된 명령문(Instructions)을 순서대로 실행하면서 만들어진다. 이 때, 이 레이어들은 각각 독립적으로 저장되고, 읽기 전용이기 때문에 임의로 수정할 수 없다.# syntax=docker/dockerfile:1FROM ubuntu:15.04COPY . /appRUN make /appCMD python /app/app.py각 명령은 하나의 레이어를 생성한다. FROM : ubuntu:15.04 Docker 이미지에서 레이어를 생성한다. COPY : Docker 클라이언트의 현재 디렉터리에서 파일을 추가한다. RUN : make 명령문을 통해 애플리케이션을 빌드한다. CMD : 컨테이너 내에서 실행할 명령을 지정한다.Docker 이미지를 실행하고 컨테이너를 생성할 때 기본 레이어(Image layers(R/O)) 위에 컨테이너 레이어(Container layer(R/W))라고도 하는 쓰기가 가능한 새 레이어를 추가한다. 즉, 아무리 많은 Docker 컨테이너를 실행하더라도 기존 읽기 전용 레이어(Image layer)는 변하지 않고, 컨테이너 마다 생성된 쓰기 가능 레이어(Container layer)에 데이터가 쌓이기 때문에 서로 겹치지 않는다. 새 파일 작성, 기존 파일 수정 및 파일 삭제와 같이 실행 중인 컨테이너에 대한 모든 변경 사항은 컨테이너 레이어에 기록된다.이처럼 Docker 이미지 레이어는 이미지를 빌드할 때마다 캐시 되어 재사용 되기 때문에 빌드 시간 단축에 있어 중요한 역할을 한다. 단, 모든 명령문이 레이어가 되는 것은 아니다.RUN, ADD, COPY 명령문이 레이어로 저장되고, CMD, LABEL, ENV, EXPOSE 등과 같이 메타 정보를 다루는 부분은 임시 레이어로 생성되지만 저장되지 않아 Docker 이미지 크기에 영향을 주지 않는다.1.2 레이어(layer) 수 줄이기Docker 이전 버전에서는 이미지 레이어 개수가 성능에 영향을 주었지만 현재는 그렇지 않다. 하지만, 레이어 수를 줄이는 것은 최적화 측면에서 도움이 될 수 있다.레이어로 저장되는 RUN, ADD, COPY 명령문에서는 여러 개로 분리된 명령을 체이닝(chaining)으로 엮어 레이어 수를 줄일 수 있다.# Before chaining (4 Layers)RUN apt-get updateRUN apt-get -y install gitRUN apt-get -y install localesRUN apt-get -y install gcc# After chaining (1 Layer)RUN apt-get update &amp;&amp; apt-get install -y \\ gcc \\ git \\ locales비록 레이어 개수가 적다고 Docker 이미지/컨테이너 성능에 영향을 주지 않지만 Dockerfile 의 가독성과 유지 보수 관점에서는 도움이 된다. 또한, 설치할 패키지를 알파벳 순으로 정렬한다면 가독성과 중복 방지 차원에서도 도움이 될 수 있다.2. 명령문(Instrucntions) 정렬위에서 언급한 것 처럼 Docker 이미지를 빌드할 때 Dockerfile에 정의된 명령문(Instructions)을 순서대로 실행한다. 따라서, 명령문을 잘 정렬하는 것 만으로도 Dockerfile을 최적화할 수 있다.아래 Dockerfile 에서는 COPY 가 2번 실행된다. 첫 번째는 의존성 패키지가 명시된 파일, 두 번째는 애플리케이션 코드가 저장된 디렉터리다.FROM python:3.8-slim-busterWORKDIR /usr/src/appCOPY requirements.txt /usr/src/appCOPY django_project /usr/src/appRUN pip install -r requirement.txtCMD [\"pip\", \"freeze\"]의존성 패키지의 경우 애플리케이션 코드에 비하여 자주 바뀌지 않기 때문에, 한 번 COPY 명령으로 생성된 레이어는 캐시되어 재사용 될 것이다. 하지만, 애플리케이션 코드의 경우 빌드할 때 마다 코드가 변경될 가능성이 높기 때문에 캐시가 자주 초기화 될 것이고, 이후 실행될 RUN 명령어에서 수행하는 의존성 패키지 설치 또한 매번 실행될 가능성이 높다. 이 경우에는 사용자가 의존성 파일(requirement.txt)에 버전을 명시하지 않거나(*) 또는 최신 상태(latest)로 명시한 경우 예기치 않은 패키지 버전의 변동이 발생할 수 있다. 따라서, 자주 변경되지 않는 COPY 명령은 애플리케이션 코드 복사 이전에 배치 하는 것이 Docker 이미지 빌드 시간을 단축하는데 유리하다.3. 패키지 최소화Docker 이미지의 크기를 최소화 하기 위해서는 불필요한 패키지를 설치하지 않거나 삭제 해야 한다.3.1 애플리케이션 패키지 최소화프로덕션 환경과 개발 환경의 패키지를 별도로 분리하여 프로덕션 환경에서는 개발용 패키지를 설치하지 않도록 설정한다.Pipfile[[source]]url = \"https://pypi.org/simple\"verify_ssl = truename = \"pypi\"[packages]fastapi = \"*\"websockets = \"*\"celery = {extras = [\"redis\"], version = \"*\"}uvicorn = \"*\"asgiref = \"*\"[dev-packages]pytest = \"*\"[requires]python_version = \"3.11\"python_full_version = \"3.11.3\"프로덕션 환경에서는 pipenv install 개발 환경에서는 --dev 옵션을 사용하여 dev-pacakages 만 설치할 수 있다.# 프로덕션 환경pipenv install # 개발 환경pipenv install --dev3.2 OS 패키지 최소화우분투(Ubuntu)를 포함한 데비안(Debian) 계열의 리눅스에서 쓰이는 패키지 관리 명령어 도구인 apt-get 를 사용할 때, --no-insatll-recommends 옵션을 사용하면 추천 패키지를 설치 하지 않는다.RUN apt-get update &amp;&amp; \\ apt-get upgrade &amp;&amp; \\ apt-get install -y --no-install-recommends netcat또한, apt-get 을 사용해서 패키지를 설치하면 이 과정에서 생성된 캐시가 디렉토리에 저장된다. 따라서 apt-get 을 통해 패키지를 설치한 이후 해당 캐시를 삭제하는 것이 좋다.RUN apt-get update &amp;&amp; \\ apt-get upgrade &amp;&amp; \\ apt-get install -y --no-install-recommends netcat &amp;&amp; \\\t\trm -rf /var/lib/apt/lists/*Reference https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#minimize-the-number-of-layers https://jonnung.dev/docker/2020/04/08/optimizing-docker-images/#gsc.tab=0 https://thearchivelog.dev/article/optimize-docker-image/" }, { "title": "[Python] Python 비동기 asyncio 사용하기", "url": "/posts/python-async/", "categories": "Programming, Python", "tags": "Python, asyncio, GIL", "date": "2023-05-05 00:00:00 +0900", "snippet": "목차 Python WAS CGI, FastCGI WSGI ASGI Python 비동기란? 코루틴 네이티브 코루틴 GIL 이란? 왜 asyncio를 사용해야 하는가? asyncio 사용하기 async/await asyncio.create_task() asyncio.gather() loop.run_inexecutor() Reference1. Python WAS?Python에서는 Tomcat과 같은 WAS가 별도로 존재하지 않는다. 그렇다면 어떤 방법으로 웹서버와 Python 애플리케이션을 연결할 수 있을까?CGI, FastCGI Common Gateway Interface외부 애플리케이션과 웹 서버(Nginx, Apache 등)를 연결해주는 표준화된 프로토콜. CGI는 클라이언트의 요청이 발생할 때 마다 프로세스를 추가로 생성하고 삭제하기 때문에 오버헤드와 성능 저하의 원인이 되었다.따라서, 이를 개선하고자 FastCGI가 등장하였다. FastCGI는 몇 번의 요청이 들어와서 하나의 프로세스만을 가지고 처리한다. 즉, 메모리에 단 하나의 프로그램만을 적재하여 재활용하기 때문에 CGI에 비하여 오버헤드가 월등하게 감소한다.Java의 Tomcat 또한 FastCGI Web Server + FastCGI 방식을 채택하고 있다하지만, Python에서는 이러한 WAS별도로 존재하지 않는다. 따라서, Python 만의 별도 게이트웨이 인터페이스를 만들었는데, 그게 바로 WSGI와 ASGI 다.WSGI Web Server Gateway InterfaceWSGI는 Python 애플리케이션, 웹 서버가 통신하기 위한 인터페이스로써 CGI 패턴을 모태로 하여 만들어졌다. WSGI는 모든 요청을 한 프로세스에서 받으며, 각 요청을 콜백(callback)을 받아 처리하게 된다. 즉, WSGI는 웹 서버와 애플리케이션 사이에서 인터페이스(미들웨어) 역할을 한다.대표적인 WSGI Middleware로는 gunicorn, uWSGI가 있으며, WSGI Application으로는 Flask, django가 있다.그러나, WSGI는 Synchronous하게 작동하기 때문에 동시에 많은 Request를 처리하는데 한계가 존재함 Celery, Queue를 이용하여 비동기적 요청에 대한 성능 향상을 할 수 있다.ASGI Asynchronous Server Gateway Interface현대 웹 서비스는 점점 더 많은 양의 트래픽 처리를 요구하고 있기에, Synchronous하게 동작하는 방식으로 처리하는 데에는 한계가 있다. 이는 ASGI가 등장하게 된 배경이 되었다.ASGI는 WSGI와 비슷한 구조를 가지나 기본적으로 모든 요청을 Asynchronous로 처리하며, WSGI에서 지원하지 않는 Websocket, HTTP 2.0을 지원한다. 또한, ASGI는 WSGI와 호환된다(ASGI는 WSGI의 상위 버전)대표적인 ASGI Server로는 uvicorn가 있으며, uvicorn은 내장 모듈로 uvloop을 사용한다. uvloop는 Javascript V8에서 사용되는 비동기 모듈을 사용하고 있으며, Cython 기반으로 C++언어로 작성되어 Go, Node JS 에 준하는 성능을 제공한다고 한다. ASGI Middleware는 WSGI와 동일한 gunicorn을 사용하는 것을 볼 수 있는데, ASGI에서 gunicorn은 프로세스 매니저로서 동작하게 된다(NodeJs에서의 pm2의 역할) Django 3.0, Falcorn 3.0 부터 ASGI를 지원한다.2. Python 비동기란?Python2와 Python3를 비교했을 때, Python3에서 가장 돋보이는 특징은 비동기 프로그래밍 지원이라고 할 수 있다. Python 3.4 버전부터는 asyncio 패키지가 추가되었고, Python 3.5 버전 부터는 네이티브 코루틴(native coroutine) 지원을 위한 async/await 키워드가 추가되었다. Python에서는 제너레이터 기반의 코루틴과 구분하기 위해 async def로 만든 코루틴은 네이티브 코루틴(native coroutine) 이라고 한다.2.1 코루틴def add(a, b): c = a + b # add 함수가 끝나면 변수와 계산식은 사라짐 print(c) print('add 함수') def calc(): add(1, 2) # add 함수가 끝나면 다시 calc 함수로 돌아옴 print('calc 함수') calc()위 소스 코드에서 calc 함수와 add 함수는 메인 루틴(main routine) 과 서브 루틴(sub routine) 관계를 가지고 있으며, 도식화 하면 아래 그림과 같이 나타낼 수 있다.Python 코루틴(coroutine)은 메인 루틴과 서브 루틴처럼 종속된 관계가 아닌 서로 대등한 관계이며, 특정 시점에 상대방의 코드를 실행한다.코루틴은 일반 함수와 달리 종료되지 않은 상태에서 메인 루틴의 코드를 실행한 뒤 다시 돌아와서 코루틴의 코드를 실행할 수 있으며, 종료되지 않은 상태로 대기 하기 때문에 코루틴의 내용도 계속 유지된다.def sum_coroutine(): total = 0 while True: x = (yield total) # 코루틴 바깥에서 값을 받아오면서 바깥으로 값을 전달 total += x co = sum_coroutine()print(next(co)) # 0: 코루틴 안의 yield까지 코드를 실행하고 코루틴에서 나온 값 출력 print(co.send(1)) # 1: 코루틴에 숫자 1을 보내고 코루틴에서 나온 값 출력print(co.send(2)) # 3: 코루틴에 숫자 2를 보내고 코루틴에서 나온 값 출력print(co.send(3)) # 6: 코루틴에 숫자 3을 보내고 코루틴에서 나온 값 출력2.2 네이티브 코루틴비동기 코루틴은 기본적으로 def 앞에 async를 붙여서 사용한다. 그리고 내부에서 다른 비동기 작업을 호출하게 되면 await를 붙여야 한다. 또한 await는 기다리는 동안 스케줄링 된 다른 작업으로 전환이 가능해야 하므로 async def 로 정의된 블럭 내에서만 사용할 수 있다.async def 를 써서 정의하는 함수를 코루틴 함수라 하며 코루틴 함수를 호출하면 (비동기) 코루틴 객체를 얻을 수 있다. await 표현식은 coroutine의 실행을 일시 중지하며, 해당 작업이 완료될 때 까지 기다린다.import asyncioasync def main(): print(\"hello\") await asyncio.sleep(1) print(\"world\") asyncio.run(main())3. GIL 이란?Python은 GIL(Global Interpreter lock) 이라는 규칙이 존재한다. GIL을 이해하려면 먼저 Python 인터프리터가 무엇인지 알아야 한다. Python 인터프리터란, Python으로 작성된 코드를 한 줄씩 읽으면서 실행하는 프로그램을 뜻한다. 그러면 본격적으로 Python GIL 개념에 대해서 알아보도록 하겠다. Python 위키에 따르면 GIL 정의는 다음과 같다. In CPython, the global interpreter lock, or GIL, is a mutex that protects access to Python objects, preventing multiple threads from executing Python bytecodes at once. This lock is necessary mainly because CPython’s memory management is not thread-safe.해석하자면, 한 프로세스 내에서 Python 인터프리터는 한 시점에 하나의 쓰레드에 의해서만 실행될 수 있다. 즉, Python에서는 멀티 쓰레드를 사용하더라도 병렬처리가 불가능하다. multiprocessing 패키지를 사용하면 병렬 처리가 가능하다4. 왜 asyncio를 사용해야 하는가?Python 에서는 GIL으로 인해 연산(CPU)의 경우 일반적으로 멀티 스레드가 싱글 스레드 보다 느리다. 하지만, 연산 작업이 아닌 파일 읽고 쓰기, HTTP 통신 대기, DB Query와 같은 Blokcing I/O의 경우에 asyncio 모듈을 사용하면 이벤트 루프(event loop)를 사용하여 비동기 작업을 처리하기 때문에 스레드를 사용하지 않는다. 즉, 비동기 작업을 처리하는 동안 내부 이벤트 루프에서는 GIL에 의한 제약이 없기 때문에 대기 시간 동안 다른 작업을 처리할 수 있어 높은 처리량과 속도를 얻을 수 있다. asyncio 모듈은 기본적으로 단일 스레드에서 실행되며, 모든 작업이 이벤트 루프에서 처리된다. asyncio에서 사용되는 스레드는 스레드 풀에서 사용하는 스레드가 아닌, asyncio 내부적으로 관리되는 스레드를 사용한다.5. asyncio 사용하기asyncio (Asynchronous I/O)는 비동기 프로그래밍을 위한 모듈이며 단일 스레드(Single Thread)를 사용하여 CPU 작업과 I/O를 병렬로 처리 즉, 동시(concurrency)에 실행할 수 있게 해준다. 이 때, 단일 스레드 동시성 모델(single-threaded concurrency model)을 위해 asyncio 패키지는 이벤트 루프(Event loop)라는 구조를 사용하고 있다. 단일 스레드 동시성 모델에서는 항상 단일 스레드로 동작하며, I/O Bound 작업을 만나면 OS의 이벤트 알림 시스템에 전달하고 다른 작업(코드)를 실행한다. I/O Bound 바운드 작업을 추적하기 위해 asyncio 패키지는 이벤트 루프를 사용한다.이벤트 루프의 동작 원리는 다음과 같다. 먼저 메인 스레드(Main Thread)는 태스크(Task)를 태스크 큐(Task Queue)에 제출한다. 이벤트 루프(Event Loop)는 태스크 큐를 지속적으로 모니터링하고 I/O 작업들이 나타날 때 까지 작업을 실행한다. I/O 작업이 발생하는 경우 이벤트 루프는 작업을 일시 중지하고 OS에 넘긴다. 이벤트 루프는 완료된 I/O 작업을 확인한다. 작업이 완료되면 OS가 프로그램에 알려, 이벤트 루프가 중지되지 않은 작업을 실행한다. 위 과정을 작업 대기열이 비워질 때까지 반복한다. Python 3.7 이후 asyncio 패키지는 이벤트 루프를 자동으로 관리할 수 있는 기능을 제공하므로 하위 수준 API를 처리할 필요가 없다.6. async/awaitPython에서 coroutine은 return에 도달하기 전에 실행을 일시 중지할 수 있는 함수이며, 일정 시간 동안 다른 coroutine에 간접적으로 제어를 전달할 수 있다.Python에서 coroutine을 만들고 일시 중지하기 위해서는 async, await 키워드를 사용할 수 있다. async는 코루틴을 생성하고, await는 코루틴을 일시중지한다.Python 3.7 이상 부터 asyncio.run() 함수를 통해 이벤트 루프를 자동으로 생성하고 코루틴을 실행한 후 닫을 수 있다. asyncio.run() 함수는 프로그램의 다른 코루틴 또는 함수를 호출할 수 있는 하나의 코루틴만 실행한다.await 키워드는 코루틴의 실행을 중단시킨다. 즉, await 키워드를 사용하면 해당 코루틴이 끝나고 결과 값을 리턴할 때 까지 대기하게 된다. 이 때, await 키워드는 반드시 코루틴 내부에서 사용해야 한다.import asyncioasync def square(number: int) -&gt; int: return number*numberasync def main() -&gt; None: x = await square(10) print(f'x={x}') y = await square(5) print(f'y={y}') print(f'total={x+y}')asyncio.run(main())============================x=100y=25total=1256.1 asyncio.creat_task()import asyncioimport timeasync def call_api(message, result=1000, delay=3): print(message) await asyncio.sleep(delay) return resultasync def main(): start = time.perf_counter() price = await call_api('Get stock price of GOOG...', 300) print(price) price = await call_api('Get stock price of APPL...', 400) print(price) end = time.perf_counter() print(f'It took {round(end-start,0)} second(s) to complete.')asyncio.run(main())================================Get stock price of GOOG...300Get stock price of APPL...400It took 6.0 second(s) to complete.코드를 도식화 하면 아래와 같이 표현할 수 있다. 위 코드에서는 코루틴을 이벤트 루프에 추가하지 않고, 직접 호출하는 방식으로 작성되어 있다. 즉, 코루틴 객체를 얻어 await 키워드를 사용하는 방식으로 결과를 얻고 있다.위 예제는 코루틴 객체를 얻어 await 키워드를 사용하여 실행한 결과이다(이벤트 루프에 넣지 않았음). 이는 async , await 키워드를 사용하여 비동기 방식으로 코드를 작성했지만, 동시에 실행시키지 않았다는 것을 의미한다.여러 동작들을 동시에 실행시키기 위해서는 태스크를 활용해야 한다. 태스크는 가능한 빨리 이벤트 루프에서 태스크가 실행되도록 코루틴을 예약하는 코루틴 wrapper이다.스케줄링 및 실행은 non-blocking 방식이기 때문에 작업을 생성하고 작업이 실행되는 동안 다른 코드를 실행할 수 있다. 즉, 위 예제 방식과는 달리 여러 태스크를 생성하고 동시에 이벤트 루프에서 실행되도록 스케줄링 한다는 차이점이 있다.태스크를 생성하기 위해서 asyncio.create_task() 에 코루틴을 전달해야 하며, create_task() 함수는 태스크 객체를 반환한다.import asyncioimport timeasync def call_api(message, result=1000, delay=3): print(message) await asyncio.sleep(delay) return resultasync def main(): start = time.perf_counter() task_1 = asyncio.create_task( call_api('Get stock price of GOOG...', 300) ) task_2 = asyncio.create_task( call_api('Get stock price of APPL...', 300) ) price = await task_1 print(price) price = await task_2 print(price) end = time.perf_counter() print(f'It took {round(end-start,0)} second(s) to complete.')asyncio.run(main())================================Get stock price of GOOG...Get stock price of APPL...300300It took 3.0 second(s) to complete. asyncio.run() 함수에 의해 이벤트 루프가 닫히기 전에 태스크가 완료될 수 있도록 await 키워드를 잘 사용해야 한다.6.2 asyncio.gather()asyncio.gather() 함수를 사용하면 여러 비동기 작업을 한 번에 실행하고 결과를 얻을 수 있다.gather(*aws, return_exceptions=False) -&gt; Future[tuple[()]]asyncio.gather() 함수는 두 개의 파라미터를 받고 있다. aws: aws 는 awaitable 객체의 묶음이다. aws의 객체가 코루틴인 경우 asyncio.gather() 함수는 자동으로 태스크를 예약한다 return_exceptions: return_exceptions는 awaitable 객체에서 예외가 발생하면 실행을 취소하지 않고, 그 다음 태스크를 실행하도록 하는 옵션이다. 해당 옵션 값을 True 로 바꾸면, awaitable 객체에서 예외가 발생할 경우 해당 객체만 예외처리 되며 나머지 작업은 정상적으로 실행된다. asyncio.gather()는 awaitable 객체를 함수에 전달한 것과 동일한 순서의 결과를 반환한다.import asyncioclass APIError(Exception): def __init__(self, message): self._message = message def __str__(self): return self._messageasync def call_api(message, result, delay=3): print(message) await asyncio.sleep(delay) return resultasync def call_api_failed(): await asyncio.sleep(1) raise APIError('API failed')async def main(): a, b, c = await asyncio.gather( call_api('Calling API 1 ...', 100, 1), call_api('Calling API 2 ...', 200, 2), call_api_failed(), return_exceptions=True ) print(a, b, c)asyncio.run(main())==============================Calling API 1 ...Calling API 2 ...100 200 API failed6.3 loop.run_in_executor()Python 3.7 이후로는 asyncio, async/await가 추가되었기 때문에 I/O Bound된 작업을 단일 스레드에서 비동기로 처리할 수 있는 방법이 생겼다. 하지만, 대부분의 Python 내장 라이브러리 함수들은 coroutine 이 아닌 일반 함수들이며, 이들은 모두 blocking 방식으로 동작한다는 문제가 있다.따라서, asyncio 패키지는 동기 함수를 비동기로 동작할 수 있도록 이벤트 루프에서 run_in_executor() 함수를 제공한다(공식 Docs).run_in_executor()함수는 blocking함수 콜 자체를 I/O Bound 작업으로 보고, 이를 기다리는 동안 일시 중지하는 비동기 코루틴으로 구현한 것이다.import timeimport requestsimport asyncio import nest_asyncionest_asyncio.apply()async def sync_time(i): loop = asyncio.get_event_loop() await loop.run_in_executor(None, time.sleep, i)async def main() : await asyncio.gather( sync_time(1), sync_time(2), sync_time(3), ) print(f\"stated at {time.strftime('%X')}\")start_time = time.time()asyncio.run(main())finish_time = time.time()print(f\"finish at {time.strftime('%X')}, total:{finish_time-start_time} sec(s)\")========================================stated at 01:36:03finish at 01:36:06, total:3.0150146484375 sec(s)ReferencePython WSGI, ASGIConcurrency and asyncCoroutine &amp; TaskAsync I/OPython Event LoopNonblocking asynchrous coroutine" }, { "title": "[Python] __hash__ 및 __eq__를 사용한 Python 해싱 및 동등성 이해", "url": "/posts/python-hash/", "categories": "Programming, Python", "tags": "Python, built-in", "date": "2023-01-26 00:00:00 +0900", "snippet": "목차 eq 연산자 hash 연산자 Reference1. __eq__ 연산자와 ispython에서 __eq__ 연산자는 == 연산자를 오버로딩(Overloading)하는 방법이다.class SomeClass: ... def __eq__(self, other): #매개변수 재정의에 따른 오버로딩 # return True if this object # is equal to other and False # otherwise. ...만약, 사용자 지정 클래스에서 __eq__ 메서드를 구현하지 않는다면 해당 클래스에서 생성된 객체는 is 연산자와 == 연산자가 동일하게 동작한다. 즉, 객체가 동일한지 확인만 하고 클래스의 속성은 신경쓰지 않는다.class MyClass: def __init__(self, a, b): self.a = a self.b = ba = MyClass(5,10)b = MyClass(5,10)assert a != b #Trueassert a is not b #True# __eq__ 메서드 구현class MyCustomClass: def __init__(self, a, b): self.a = a self.b = b def __eq__(self, other): return self.a == other.a and self.b == other.ba = MyCustomClass(5,10)b = MyCustomClass(5,10)assert a == b #Trueassert a is not b #True2. __hash__ 연산자Python의 hash() 함수는 built-in 함수이며 개체에 해시 값이 있는 경우 해당 개체의 해시 값을 반환한다. 이 때, 해시 값은 딕셔너리(dict)를 보면서 빠르게 키를 비교하는 데 사용되는 정수이다.Python의 딕셔너리(dictionary)는 hash map 이며, set 또는 frozenset도 이 hash map을 사용하여 구현된다. Python에서 __hash__ 함수는 set과 dict에서 Key를 해싱하기 위한 해시함수로 사용되고 있으며, __eq__와 마찬가지로 클래스에 추가할 수 있는 메서드이다.class Example: def __init__(self, a): self.a = a def __hash__(self): return self.aex = Example(3)hash(ex) #3hash() 함수의 속성 hash()를 사용하여 해시된 개체는 되돌릴 수 없다(단방향성). hash()는 불변 객체에 대해서만 해시 값을 반환하므로 가변/불변 객체를 확인하는 지표로 사용할 수 있다. 가변(mutable) 객체: list, set, dict 불변(immutable) 객체: int, float, bool, tuple, string, unicode 두 객체가 같다면(__eq__가 True 라면) 동일한 __hash__ 값을 가져야 한다(그러나 반대의 경우는 반드시 참은 아니다) __hash__를 정의하지 않고 __eq__만 정의한다면, 해당 개체는 해시화 할 수 없다. __eq__ 메서드가 가변/변경 가능한 속성을 사용하는 경우 __hash__ 메서드를 정의해서는 안된다.아래와 같은 상수 값을 가지는 해시는 해시 테이블에 단일 버킷만 존재하여 충돌을 야기한다. 즉, 일정한 해시를 사용하면 단일 버킷의 항목이 반복되고 일치 항목이 발견될 때 까지 속성 값을 비교해야 하기 때문에 해시 테이블의 데이터 삽입 시간 복잡도는 O(n^2)이 된다.class Example: def __init__(self, a): self.a = a def __eq__(self, other): return self.a == other.a def __hash__(self): return 1a = Example(1)b = Example(2)obj = { a: 10, b: 20 }# It works:print(obj[Example(1)])# =&gt; 10print(obj[Example(2)])# =&gt; 20위 상황을 해시 테이블로 도식화 해보면 아래와 같이 나타날 것이다. 즉, 상수 값을 가지고 있는 해시 테이블의 경우 인덱스 값이 동일하기 때문에 __eq__ 메서드를 활용하여 속성 값이 일치하는지 모두 확인해야 한다. hash value Actual value 1 [a, b] 2 NULL 3 NULL hash() 함수에 대해 간략하게 작성하려고 했는데, 막상 자료를 찾다보니 자료구조에 대한 부족함을 많이 느꼈다. 따라서, 다음 포스팅에서는 이번 글을 쓸 때에 잘 이해가 되지 않았던 해시 테이블에 대해서 작성하도록 하겠다 !ReferenceUnderstanding Hashing and Equality in Python with hash and eqpython hashhash 테이블 동작 원리" }, { "title": "[파이썬으로 살펴보는 아키텍처 패턴] Domain model", "url": "/posts/architecture-domain/", "categories": "Programming, Architecture", "tags": "Architecture, Python, Domain", "date": "2023-01-15 00:00:00 +0900", "snippet": "파이썬으로 살펴보는 아키텍처 패턴을 읽고 배운점을 기록하도록 하겠다.이 책의 목적은 여러 가지 고전적인 아키텍처 패턴을 소개하고 이런 패턴들이 어떻게 DDD, TDD, 이벤트 기반 서비스를 지원하는지 파이썬을 통해 보여주는 것이다.목차 도메인 모델링 도메인 모델이란 값 객체와 엔티티 도메인 서비스 도메인 모델링 정리 1. 도메인 모델링 대부분 개발자가 새로운 시스템을 설계하라는 요청을 받으면 즉시 데이터베이스 스키마를 그리기 시작하고 그 다음 객체 모델을 생각한다.이 책을 읽기 전까지는 이 과정이 당연하다고 생각했다. 하지만, 책에 따르면 새로운 시스템을 설계하기 위해서는 먼저 행동하고 저장에 대한 요구 사항은 행동에 맞춰 정해져야 한다고 한다.이번 포스팅에서는 도메인 모델링(domain modeling)이 왜 중요하며, 도메인을 모델링하기 위한 핵심 패턴인 엔티티(entity), 값 객체(value object), 도메인 서비스(domain service)에 대해 작성하도록 하겠다.1.1 도메인 모델이란?일반적으로 알려져 있는 3계층 아키텍처(3-layer-architecture)에서 비즈니스 로직 계층(business logic layer)은 아키텍처의 핵심 계층을 뜻한다. 이 책에서는 해당 용어를 도메인 모델로 정의하여 사용 한다.도메인 이라는 용어는 처음들었을 때, 굉장히 추상적이고 생소하게 느껴졌다. 하지만, 그럴 필요 없이 도메인은 우리가 해결하는 문제라고 정의할 수 있겠다. 예를 들어, 온라인 가구 판매회사에서 일하고 있는 사람에게는 구매 및 조달, 제품 설계, 물류 및 배달 등이 도메인이 될 수 있다.도메인 모델은 비즈니스를 수행하는 사람이 자신의 비즈니스에 대해 마음속에 가지고 있는 지도와 같다. 즉, 인간이 복잡한 프로세스에 대해 생각하는 방식을 모델로써 표현한 것이다. 이 책에서는 도메인 모델을 만드는 기본적인 방법을 보여준다. 이로써 모델을 외부 제약과 최대한 무관하게 유지할 수 있고, 이로 인해 모델이 더 잘 진화하고 변경될 수 있게 해주는 아키텍처를 보여준다.1.2 값 객체와 엔티티값 객체는 데이터는 있지만 유일한 식별자가 없는 비즈니스 개념을 뜻한다. 즉, 안에 있는 데이터에 따라 유일하게 식별될 수 있는 도메인 객체를 의미한다. 보통 값 객체는 불변 객체(immutable object)로 만들곤 한다.예를 들어, 주문(Order) 내에 포함되어 있고, 제품(SKU)과 수량으로 구성된 라인(line)은 각 라인에 있는 데이터에 따라 유일하게 식별될 수 있다.파이썬에서 데이터 클래스(or namedtuple)을 사용하여, 값 동등성(value equality)을 부여할 수 있다(즉, 클래스의 속성값이 같은 두 라인은 같다).@dataclass(frozen=True) #immutable objectclass OrderLine: order_id: OrderRefernce sku: ProductReference qty: Quantity엔티티는 값 객체와 달리 유일하게 구별할 수 있는 식별자가 존재한다. 즉, 오랫동안 유지되는 정체성이 존재하는 도메인 객체를 설명할 때 엔티티 용어를 사용한다.예를 들어, 사람은 자신의 이름이나 결혼 상태 그리고 성별도 바꿀 수 있다. 하지만, 이런 변경에도 모두 같은 사람으로 계속 인식할 수 있다. 즉, 사람은 영속적인 정체성(persistent identity)이 있다.엔티티에는 정체성 동등성(identity equality)이 있다. 엔티티의 값을 바꾸더라도 바뀐 엔티티는 이전과 같은 엔티티로 인식된다.만약, 특정한 이유로 엔티티를 집합에 넣거나 딕셔너리의 키로 사용해야 한다면 시간과 무관하게 엔티티의 정체성을 식별해주는 속성을 사용해 해시를 정의해야 한다. 그리고 이 속성을 읽기 전용으로 만들어야 한다.1.3 도메인 서비스도메인 서비스는 비즈니스 개념이나 프로세스를 표현하며, 엔티티나 값 객체로 자연스럽게 표현할 수 없는 개념이다.예를 들어, “배치 집합(엔티티)에 대해 주문 라인(값 객체)을 할당하는 행위”가 도메인 서비스에 해당된다.1.4 도메인 모델링 정리도메인 모델링비즈니스와 가장 가까운 부분으로 변화가 생길 가능성이 높고, 비즈니스에 가장 큰 가치를 제공하는 부분엔티티와 값 객체 구분값 객체는 내부 속성들에 의해 정의되며, 불변 타입을 사용해 값 객체를 구현하는 것이 좋다.엔티티는 속성이 바뀌더라도 여전히 똑같은 엔티티로 남는다. 엔티티는 주로 이름이나 참조 번호를 사용하여 식별한다.모든 것을 객체로 만들 필요가 없다파이썬 코드에서 ‘동사’에 해당하는 부분을 표현하기 위해 항상 객체로 만들기 보다는 함수를 사용하기는 것이 가독성이 더 좋고 표현력이 좋다.설게 원칙의 적용SOLID 원칙이나 has-a와 is-a의 관계, 상속(ingeritance)보다는 구성(composition)을 사용하라" }, { "title": "[Python] Iterator and Generator", "url": "/posts/python-iterator/", "categories": "Programming, Python", "tags": "Iterator, Generator, Python", "date": "2023-01-10 00:00:00 +0900", "snippet": "이 번 포스팅에서는 iterator와 generator에 대해서 알아보도록 하겠다.목차 Iterator? Iterator 만들기 iter next Generator? Generator 만들기 yield from 1. Iterator?이터레이터(iterator)는 값을 차례대로 꺼낼 수 있는 객체(object) 또는 반복자를 말한다. for 반복문을 사용할 때 흔히 range() 를 활용해 연속된 숫자를 만들어 낸다고 생각하지만, 사실 숫자를 차례대로 꺼낼 수 있는 이터레이터가 내부에서 동작하고 있는 것이다.이처럼 Python에서는 이터레이터만 생성하고 값이 필요한 시점이 되었을 때 값을 만드는 방식을 사용하고, 이를 지연 평가(lazy evaluation)라고 한다.문자열, 리스트, 딕셔너리, 세트 등 반복이 가능한 객체는 내부 magic method에 __iter__ 메서드가 포함되어 있다. 이 때, 반복이 가능한 객체에서 __iter__ 를 호출하면 이터레이터 객체를 호출할 수 있다. 또한, __next__ 메서드를 사용하면 반복 가능한 객체 내의 요소를 차례대로 꺼낼 수 있으며, 이터레이터 내 모든 요소가 다 호출되면 StopIteration 예외를 발생시킨다.it = [1,2,3].__iter__()it.__next__()1it.__next__()2딕셔너리와 세트는 반복 가능한 객체이지만 순서가 정해져 있지 않기 때문에, 시퀀스 객체에 포함되지 않는다.1.1 Iterator 만들기class Counter: def __init__(self, stop): self.current = 0 self.stop = stop def __iter__(self): #현재 인스턴스 반환 return self def __next__(self): if self.current &lt; self.stop: r = self.current self.current += 1 return r else: raise StopIteration위 객체는 리스트, 문자열, 딕셔너리, 세트, range 처럼 __iter__ 를 호출해줄 반복 가능한 객체(iterable)가 없으므로 __iter__ 메세드에서 현재 인스턴스를 반환 해준다.class Counter: def __init__(self, stop): self.current = 0 self.stop = stop def __getitem__(self, index): if index &lt; self.stop: return index else: raise IndexError위 과정을 조금 더 간단하게 나타낼 수 있는 방법으로는 __getitem__ 메서드를 사용하는 방법이 있고, 이를 구현하면 인덱스로 접근할 수 있는 이터레이터가 생성된다. __getitem__ 메서드를 사용하는 경우 __iter__, __next__ 를 생략할 수 있다.python 내장 함수에서는 iter(), next()를 각각 지원한다.1.2 iteriter는 반복을 끝낼 값을 지정하면 특정 값이 나올 때 반복을 끝낸다. iter(호출가능한 객체, 반복을 끝낼 값)이 때, 기존과 다른 점으로는 반복 가능한 객체가 아닌 호출가능한 객체를 입력받다는 것이 있다import randomit = iter(lambda : random.randint(0,5), 2)next(it)3next(it)1next(it)Traceback (most recent call last): File \"&lt;pyshell#37&gt;\", line 1, in &lt;module&gt; next(it)StopIteration1.3 nextnext는 기본값을 지정할 수 있다. 기본값을 지정하면 반복이 끝나더라도 StopIteration이 발생하지 않고 기본값을 출력한다. next(반복 가능한 객체, 기본값)it = iter(range(2))next(it, 10)0next(it, 10)2next(it, 10)10next(it, 10)102. Generator?제너레이터(generator)는 이터레이터를 생성해주는 함수이다. 이터레이터(iterator)는 클래스 내부에 __iter__, __next__ 또는 __getitem__을 구현해야 하지만, 제너레이터는 함수 안에서 yield라는 키워드만 사용하면 된다.def number_generator(): yield 0 yield 1 yield 2for i in number_generator(): print(i)012먼저 제너레이터 객체를 만든 후, next()를 호출하면 제너레이터 안의 yield 0이 실행되어 숫자 0을 전달한 뒤 바깥의 코드가 실행되도록 양보한다. yield는 함수를 끝내지 않은 상태에서 값을 함수 바깥으로 전달할 수 있다.2.1 Generator 만들기def number_generator(stop): n = 0 while n &lt; stop: yield n n += 1def i in number_generator(3): print(i)yield에서는 변수 뿐만 아니라 함수도 호출할 수 있다. 이 때, yield에서 함수(메서드)를 호출하면 해당 함수의 반환값을 바깥으로 전달한다.def upper_generator(x): for i in x: yield i.upper() # 함수의 반환값을 바깥으로 전달 fruits = ['apple', 'pear', 'grape', 'pineapple', 'orange']for i in upper_generator(fruits): print(i) #모두 대문자로 출력2.2 yield fromyield from을 사용하면 반복 가능한 객체를 대상으로 요소를 한 개씩 바깥으로 전달 할 수 있다.def number_generator(): x = [1, 2, 3] yield from x # 리스트에 들어있는 요소를 한 개씩 바깥으로 전달 for i in number_generator(): print(i)또한, yield from에는 제너레이터 객체를 지정하는 것도 가능하다.number_generator(3)은 숫자를 세개 만들어 내므로 yield from 역시 숫자를 세 번 바깥으로 전달한다.def number_generator(stop): n = 0 while n &lt; stop: yield n n += 1 def three_generator(): yield from number_generator(3) # 숫자를 세 번 바깥으로 전달 for i in three_generator(): print(i) 리스트 표현식을 []가 아닌 ()묶으면 제너레이터 표현식이 된다. 이는 필요할 때 마다 요소를 만들어내므로 메모리를 절약할 수 있다." }, { "title": "[FastAPI] Session deadlock issue", "url": "/posts/python-dead-lock/", "categories": "Programming, Python", "tags": "FastAPI, Session, deadlock, SQLAlchemy", "date": "2022-08-31 00:00:00 +0900", "snippet": "production 배포 전 동시성 테스트 단계에서 백엔드 서버가 request를 처리하지 못하고, 교착 상태(deadlock)로 멈추고 아래와 같은 에러를 뱉는 이슈가 지속적으로 발생했다. 교착 상태(deadlock)란?두 개 이상의 작업이 서로 상대방의 작업이 끝나기만을 기다려, 결과적으로 아무것도 완료되지 못하는 상태를 가리킴sqlalchemy.exc.TimeoutError: QueuePool limit of size 5 overflow 10 reached결론부터 말하자면, 이번 issue는 FastAPI가 외부 Thread Pool(ThreadPoolExecutor)을 제어하는데 기반으로 하는 starlette의 anyio worker thread와 SQLAlchemy의 session.execute blocking call로 인해 발생하였다.이번 포스팅에서는 anyio worker thread와 session.execute가 무엇이고, 왜 이러한 이슈가 발생했는지 그리고 어떻게 해결 했는지에 대하여 작성해 보겠다.목차 교착 상태, 왜 발생했는가? 해결 방안 Thread, session engine 활용 Native Corutine 활용 의존성 제거 문제 톺아보기 References1. 교착 상태, 왜 발생했는가?FastAPI 공식 Github issue에서 문제에 대한 다양한 의견들을 찾아보았고, 그 중 해당 issue의 본질적인 원인에 대하여 구체적으로 잘 설명한 답변이 있어 이를 참조하였다.간단한 코드를 통해 설명하겠다. import time import uvicorn from fastapi import Depends, FastAPI, Request from sqlalchemy import create_engine from sqlalchemy.orm import Session, sessionmaker from sqlalchemy.pool import QueuePool from anyio.lowlevel import RunVar from anyio import CapacityLimiter # SQLAlchemy setup engine = create_engine( 'sqlite:///test.db', connect_args={'check_same_thread': False}, poolclass=QueuePool, pool_size=4, max_overflow=0, pool_timeout=None, # Wait forever for a connection ) SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine) # FastAPI app = FastAPI() def get_db(request: Request): db = SessionLocal() try: yield db finally: db.close() @app.get('/') def index(db: Session = Depends(get_db)): # Some blocking work _ = db.execute('select 1') time.sleep(1) return {'hello': 'world'} # Run if __name__ == '__main__': uvicorn.run('app:app', reload=True, host='0.0.0.0', port=80) jmeter 또는 locust를 사용하여, thread pool size 이상의 requests가 요청 되었다고 가정 모든 요청이 들어왔을 때, 요청들은 의존성 주입(Depends)을 통해 세션 객체를 생성(SessionLocal())하고, yield 키워드를 사용한다. 이 후 path operation function에서 실행되는 db.execute 는 blocking operation으로 SQLAlchemy docs에 따르면 세션은 쿼리가 실행되는 시점에서 connection pool에서 연결을 요청한다. 위 예제에서는 SQLAlchemy connection pool 크기를 4로 설정했기 때문에, 4개 요청 이후에 실행된 db.execute 요청들은 연결되기를 기다리는 상태에 머무른다(blocking) 4 개의 요청이 모두 checked out 된 상태 FastAPI에서는 사용자가 path operation function을 def로 선언할 경우, 직접 호출되지 않고 외부 스레드 풀에서 실행된다. 이 때, anyio의 스레드 풀 사용하며, anyio 스레드 풀의 경우 40개의 worker thread가 기본값으로 설정되어 있다. 스레드 풀의 갯수를 변경하고 싶은 경우 아래와 같이 변경할 수 있다. from fastapi import FastAPI from anyio.lowlevel import RunVar from anyio import CapacityLimiter app = FastAPI() @app.on_event(\"startup\") def startup(): # N : number of threads N = 50 RunVar(\"_default_thread_limiter\").set(CapacityLimiter(N)) session과 연결되어 있는 4개의 요청이 작업을 마치게 되면, path operation function response 이후 get_db()의 finally 블록이 실행된다. 이 때, FastAPI는 finally블록을 실행시킬 때 __exit__ 메서드를 호출하여 세션을 정리한다. 위 예제에서 get_db() 역시 def로 선언되어 있기 때문에, path operation function response 이후 코루틴이 아닌 외부 스레드 풀에서 실행된다. 하지만, 모든 anyio worker thread는 SQLAlchemy 연결을 기다리는 상태에 놓여있기 때문에 4개의 요청을 connection pool로 반납하지 못하는 교착 상태에 놓이게 된다. 즉, 위 예제 코드의 근본적인 문제는 anyio worker thread 사용과 관련이 있다는 것을 알 수 있다.2. 해결 방안위 문제를 해결할 수 있는 방법으로는1. anyio worker thread 갯수와 connection pool 크기 확장 #number of threads = 100 RunVar(\"_default_thread_limiter\").set(CapacityLimiter(100)) #connection pool size = 100 engine = create_engine( 'sqlite:///test.db', connect_args={'check_same_thread': False}, poolclass=QueuePool, pool_size=25, max_overflow=75, pool_timeout=None, # Wait forever for a connection ) Thread갯수를 늘리거나 engine 생성 시 pool_size의 크기와 pool_timeout 시간을 조절하여 문제를 해결할 수도 있다. 하지만, 이 방법은 서버의 리소스를 낭비할 수 있으며, pool_timeout(connection 지속 시간)으로 인하여 데이터가 제대로 처리되지 않고 유실될 수 있다는 큰 문제점을 가지고 있다. 따라서, 이 방식은 채택하지 않았다.2. path operation function과 dependency function을 모두 async로 선언하여 native corutine으로 동작시키기 async def get_db(request: Request): async def index(db: Session = Depends(get_db)): MySQL 드라이버로 mysql+pymysql(동기식)을 사용하고 있기 때문에 채택하지 않았다 MySQL 드라이버가 비동기를 지원하지 않더라도 async로 함수를 선언하여 사용할 수는 있다. 하지만, 퍼포먼스 측면에서 보았을 때, async + sync 조합은 교착 상태 발생의 원인이 되므로 권장하지 않는 방법이다.Sync/Async Function 이슈 3. 의존성 주입(Depends)을 제거하고 Session 객체를 직접 호출하기 교착 상태를 해결할 수 있는 다양한 해결 방법을 찾던 중, 원티드 FastAPI boilerplate를 발견하게 되었고, Github 소스 코드를 통해 아이디어를 얻을 수 있었다. 해결 방법은 이렇다. path operation function에서 의존성 주입(Depends)를 통해 세션 객체를 얻어왔던 기존 방식에서 실제 DB에 CRUD 작업이 이루어지는 곳에서 세션 객체를 다루는 방식으로 변경하였다. ##### Server ##### # path : fastapi-boilerplate/app/serverpy from fastapi import FastAPI app = FastAPI() ##### Session ##### # path : fastapi-boilerplate/core/db/session.py from sqlalchemy import create_engine from sqlalchemy.orm import scoped_session, sessionmaker, Session engine = create_engine(config.DB_URL, pool_recycle=3600) session: Union[Session, scoped_session] = scoped_session( sessionmaker(autocommit=True, autoflush=False, bind=engine), scopefunc=get_session_id, ) ##### Service ##### # path : fastapi-boilerplate/app/services/user.py from core.db import session #세션을 직접 호출하여 사용 async def get_user_list(self, limit: int, prev: Optional[int]) -&gt; List[User]: query = session.query(User) if prev: query = query.filter(User.id &lt; prev) if limit &gt; 10: limit = 10 현재 개발 중인 서비스 특성상 외부 Third party API와 통신 작업과 데이터 파싱 작업이 많아 응답까지의 소요 시간이 큰 편이다. 즉, 이러한 상황에서 의존성 주입(Depends)은 아주 큰 독이 될 수 있다. 따라서, 의존성 주입(Depends)을 제거하고, DB에 CRUD 작업이 이루어지는 곳에서 세션 객체를 직접 호출하는 방식을 채택하였다. 문제 톺아보기사실 아직까지도 FastAPI GitHub issue 페이지에서 해당 이슈에 대해서 많은 사람들의 의견을 시간 날 때 마다 보고있다. 글을 곱씹을수록 어제까지만 하더라도 잘 이해되지 않던 개념이 이해가 되고, 조금 더 나은 방식으로 수정 할 수 있겠다 라는 생각이 들어 글을 자꾸 읽게 되는 것 같다.그래서 ‘과연 이 해결 방안이 최선인가?’ 라는 질문을 해보았을 때, ‘아직까지 내가 아는 한..내일은 또 모르지‘라고 답하고 싶다.References Dependency injection deadlock issue FastAPI 속도 비교 FastAPI에서 SQLAlchemy session 다루는 방법 Wanted FastAPI boilerplate" }, { "title": "[OS] 멀티 프로그래밍, 멀티 태스킹, 멀티 프로세싱, 멀티 스레딩", "url": "/posts/cs-kind-of-process/", "categories": "Study, CS", "tags": "Multi-programming, Multi-Tasking, Multi-processsing, Multi-Threading", "date": "2022-07-11 00:00:00 +0900", "snippet": "저번 포스팅에 이어, 이번 포스팅 에서는 Multi-programming, Multi-Taskng, Multi-processing, Multi-Threding을다뤄보도록 하겠다.목차 Multi-programming Multi-Tasking Multi-processsing Multi-Threading1. Multi-Programming 단일 프로세서에서 여러 프로그램을 메모리에 동시에 올려놓고 수행하는 것을 뜻한다. 멀티 프로그래밍에서는 하나의 프로그램이 I/O 작업을 하는 경우 유휴 시간으로 대기하는 것이 아닌, 다른 프로그램이 그 동안 CPU 및 기타 리소스를 활용한다.출처: https://www.scaler.com/topics/multiprogramming-operating-system/ I/O 작업이 이루어지고 있는 A작업은 CPU를 활용하지 않는다. 따라서, CPU는 B 작업을 실행시킨다. 그 동안 C 작업은 B 작업이 끝난 후, CPU 시간을 활용할 수 있도록 대기 상태에서 머무른다. 이처럼 멀티 프로그래밍은 CPU의 유휴 시간을 최소화 하여, CPU 사용을 극대화 하는데 목적을 두고 있다.2. Multi-Tasking 다수의 작업(Task)을 운영체제 스케줄링에 의해 번갈아가면서 처리하는 것을 뜻한다(Process, Thread 모두 작업의 단위가 될 수 있음). 시분할 시스템(Time Sharing System)을 적용하여 프로세스의 응답 시간을 최소화 하는데 목적을 두고 있다. 시분할 시스템이란? 한 작업이 CPU 시간을 오래 잡고 있는 것을 방지하고자, 각 작업을 일정 시간(time quantum or time slice) 동안 번갈아 가면서 실행하는 것을 뜻함 멀티 프로그래밍과 멀티 태스킹의 가장 큰 차이점은 시분할 시스템(Time Sharing System) 이다. 멀티 프로그래밍은 time slice에 의한 CPU 스위칭이 없기 때문에(I/O 스위칭만 발생) 하나의 작업이 CPU 시간을 오래 잡고 있을 수 있다는 문제가 발생한다. 이는 높은 CPU 이용률과 긴 응답 시간의 결과를 가져온다. 3. Multi-Processing 두 대 이상의 프로세서가 다수의 프로세스를 협력적으로 동시에 처리하는 것(프로세서는 CPU와 같은 개념으로 생각하면 된다) 각 프로세서는 다수의 프로세스를 처리하며, 각 프로세스는 다수의 프로세서에 의해 처리된다. 각 프로세서가 자원을 공유하면서 프로세스를 처리하기 때문에, 하나의 프로세서가 멈춰도 작업은 정지되지 않는다.출처: https://levelup.gitconnected.com/4. Multi-Threading 스레드(Thread)란 ? 한 프로세스 내에서 구분지어진 실행 단위 프로세서가 여러 개인 경우 멀티 스레딩 을 통해 병렬성(Parallelism)을 높일 수 있다. 프로세스의 스레드들이 각각 다른 프로세서(CPU)에서 병렬적으로 수행될 수 있기 때문 만약, 프로세서가 한 개인 경우에는 멀티 스레딩을 통해 동시성(Concurrency) 를 높일 수 있다. 작업 중인 스레드가 blocked(waiting) 되더라도 다른 스레드로 스위칭이 가능하기 때문에, 실제로는 각 시간에는 한 작업만 수행되지만 병렬적으로 수행되는 것 처럼 보인다. 출처: https://levelup.gitconnected.com/Reference Definition of Multiprogramming Operating System 멀티 프로그래밍 &amp; 시분할 시스템 멀티 스레드" }, { "title": "[OS] Process, Thread", "url": "/posts/cs-process/", "categories": "Study, CS", "tags": "Process, Thread, Context-Switching", "date": "2022-07-05 00:00:00 +0900", "snippet": "Front Team과 협업을 하면서 가장 번거로웠던 작업은 OpenAPI를 만드는 작업이었다. 그러던 와중 API 개발만 해도 OpenAPI를 자동으로 만들어 준다는 FastAPI라는 Python 프레임 워크를 알게되었고, 현재 다니고 있는 회사의 백엔드 프레임 워크로 사용중이다.부끄러운 이야기이지만(..😰) 개발을 하면서 소스 코드 수준에서의 리팩토링이나 알고리즘 개선은 많이 다루었지만, FastAPI 프레임 워크의 특징 및 장점들에 대해서는 잘 알지 못하고 사용하였다.잘 알지 못하고 배포한 백엔드 서버는 결국 Production 환경 배포 전 부하 테스트에서 어마어마한 에러를 토해냈다. 개발자로서 잘 알지 못하고 사용했다는 것이 굉장히 부끄러웠고, 스스로에게 큰 실망을 했다. 그래서 다시는 이러한 실수를 다시 반복하지 않기 위해, 앞으로는 다 는 아니더라도 잘 알고 쓰고자 한다.평소 CS 지식이 많이 부족했지만, 실질적으로 와닿는 부분이 없어 지나치곤 했다. 하지만, 많은 에러 경험 덕분(?)에 점점 더 근본적인 문제에 대해 고민하게 되었고, 현재 이 포스팅까지 쓰게 되었다. 이번 포스팅을 시작으로 CS 지식을 많이 습득하고자 한다. 첫 번째 주제는 Process와 Thread 로 선정했다.목차 Process Thread Context switching1. Process프로세스를 설명하기 전, 프로그램이란 무엇인지 간략하게 짚고 넘어가고자 한다. 프로그램 이란? 운영체제에 의해서 실행될 수 있는 실행 파일을 뜻한다. 즉, 코드가 구현되어 있는 파일을 프로그램으로 볼 수 있다 e.g. python file, python.exe, uvicorn 등 프로세스 란? 운영체제 위에서 CPU, Memory를 사용하며 프로그램을 실행시키는 주체 각각의 프로세스는 독립된 메모리 공간을 할당 받는다. 따라서, 프로세스 하나가 죽더라도 다른 프로세스에는 영향이 가지 않는다. 여러 개의 프로세스가 돌아 갈 때는 시분할로 돌아가며, 프로세스 간 Context switching 은 느리다. 프로세스는 스레드와 달리 공유하는 영역이 없어서 캐시 데이터를 다 버리고 다시 캐시를 만드는 과정이 발생하기 때문에 Context switching이 스레드에 비해 느리다. 출처: https://gmlwjd9405.github.io 프로세스는 각각 독립된 메모리 영역(Code, Data, Stack, Heap의 구조)을 할당받는다. 한 프로세스가 다른 프로세스 자원에 접근하려면 프로세스 간의 통신(IPC, inter-process communication)을 사용해야 한다.2. Thread 스레드 란? 프로세스 내에서 실제로 작업을 수행하는 주체를 의미함 모든 프로세스에는 한 개 이상의 스레드가 존재하여 작업을 수행 두 개 이상의 스레드를 가지는 프로세스를 멀티 스레드 프로세스(multi-threaded process)라고 하며, 프로세스 내에서 동시에 여러 작업을 실행하는데 목적을 두고 있다. 스레드는 ThreadStack 영역을 제외한 나머지 영역(Code, Data, Heap)은 공유하고 있다. 즉, 스레드는 공유하는 영역이 많기 때문에 프로세스에 비하여 Context switching이 빠르다. 그러나, 공유하는 전역 변수를 여러 스레드가 함께 사용하게되면 충돌이 발생할 수 있기 때문에 스레드 간 통신에는 충돌 문제가 발생하지 않도록 동기화 문제를 잘 해결해야 한다.출처 : https://gmlwjd9405.github.io 각각의 스레드는 별도의 레지스터와 스택을 갖고 있지만, Heap 메모리는 서로 읽고 쓸 수 있다. 한 스레드가 프로세스 자원을 변경하면, 다른 이웃 스레드(sibling thread)도 그 변경 결과를 즉시 볼 수 있다.3. Context switching Context switching 이란? 문맥교환이라고도 하며, CPU가 실행하는 작업을 변경하는 것을 말한다. 즉, 동작 중인 프로세스가 대기를 하면서 해당 프로세스의 상태(Context)를 보관하고, 대기하고 있던 다음 순서의 프로세스가 동작하면서 이전에 보관했던 프로세스의 상태를 복구하는 작업을 말한다. 그렇다면, Context switching은 언제 발생할까? 인터럽트(Interrupt) CPU가 프로그램을 실행하고 있을 때, 실행 중인 프로그램 밖에서 예외 상황이 발생하여 처리가 필요한 경우 CPU에게 알려 작동이 중단되지 않고 예외 상황을 처리할 수 있도록 하는 기능 Context switching 은 아래와 같은 인터럽트(Interupt) 요청이 왔을 때 발생한다. 입/출력 요청 CPU 사용시간 만료 자식 프로세스 Fork 스케줄러는 Context switching을 하는 주체로써 Context switching이 발생했을 때, 다음번 프로세스는 스케줄러가 결정한다. Context switching 은 인터럽트 상황에 대하여 프로세스가 유휴 상태로 대기하고 있는 것을 방지해 프로세스의 응답 시간을 단축할 수 있다. 하지만, 잦은 Context swtiching 발생은 오히려 오버헤드(Overhead) 비용을 발생시켜 성능을 떨어뜨린다. 출처: https://www.crocus.co.kr/1364 위 그림에서 프로세스 P0가 실행 중인 상태(executing)에서 유휴 상태(idle)가 될 때, 프로세스 P1이 곧바로 실행 상태로 되지 않고, 유휴 상태(idle)가 유지 되다가 변경된다. 실행 상태로 즉각 변경되지 않는 이유는 프로세스 P0의 상태를 PCB에 저장하고 프로세스 P1 상태를 PCB에서 가져와야 하기 때문이다. 이 과정에서 CPU는 아무런 동작을 하지 못하게 되고, 이는 곧 성능 저하로 이어진다.다음 포스팅에서는 멀티 프로그래밍, 멀티 태스킹, 멀티 스레딩, 멀티 프로세싱에 대해서 다루어 보도록 하겠다. 추가로, 현재 내가 사용중인 Python 의 Process , Thread 는 어떻게 구성되어 있는지도 알아볼 예정이다.Reference 멀티 프로세스와 멀티 스레드 Context switching 프로세스와 스레드 차이" }, { "title": "[Python] WSGI, ASGI", "url": "/posts/python-server/", "categories": "Programming, Python", "tags": "WSGI, ASGI, Server, Python", "date": "2022-06-26 00:00:00 +0900", "snippet": "Python 환경에서 웹 서버와 Python 애플리케이션 간의 통신이 어떻게 이루어지는가에 대한 궁금증이 생기던 와중 잘 정리되어 있는 블로그가 있어 이번 글을 작성하게 되었다.Python에는 Spring 계열의 Tomcat과 같은 WAS가 별도로 존재하지 않기 때문에 웹 서버와 애플리케이션 간의 통신을 위한 서버가 필요하다. 즉, 웹 서버와 애플리케이션 간의 인터페이스 역할을 해주는 미들웨어가 필요하다.목차 WSGI ASGI WSGI, ASGI1. WSGI Web Server Gateway InterfaceWSGI 서버는 웹 서버가 동적 페이지 요청을 처리하기 위해 호출하는 서버이며, 일반적으로 gunicorn 과 uwsgi 를 가장 많이 사용한다.WSGI 서버는 웹 서버와 WSGI 애플리케이션 중간에 위치하고 있다. 웹 서버에 동적 요청이 발생하면 웹 서버가 WSGI 서버를 호출하고, WSGI 서버는 Python 프로그램을 호출하여 동적 페이지 요청을 대신 처리한다. WSGI 서버는 WSGI 미들웨어 또는 WSGI 컨테이너라고도 한다.웹 서버로부터 발생한 동적 요청은 결국 WSGI 애플리케이션이 처리하게 된다. 대표적인 WSGI 애플리케이션으로는 Django, Flask, Tornado가 있다.1.1 WSGI 순서도2. ASGI Async Server Gateway InterfaceASGI도 WSGI와 동일하게 웹 서버 및 애플리케이션 간의 표준 인터페이스를 제공하기 위한 서버이며, 일반적으로 starlette, uvicorn 을 사용한다. uvicorn은 uvloop와 httptools를 이용하여 ASGI를 구현한 서버이다. uvloop는 NodeJS V8 엔진에서 사용하는 libuv를 기반으로 작성되어 Node.js와 같은 비동기 처리 속도를 어느 정도 누를 수 있다는 장점이 있다.다음과 같은 WSGI의 한계로 인하여 ASGI가 등장하게 되었다. wsgi.websocket을 사용할 수 있으나 표준화되지 않았다. HTTP/2(Concurrency) 동시성을 적용할 수 없다. 기본적으로 요청을 비동기로 처리하지 못한다.대표적인 ASGI 애플리케이션으로는 Django, FastAPI, Falcon(3.0 이상)가 있다.2.1 ASGI 순서도3. WSGI, ASGI WSGI와 ASGI 동시에 사용하기uvicorn(ASGI)는 단일 프로세스로 비동기 처리가 가능하지만, 많은 양의 요청이 들어오는 경우 결국 단일 프로세스라는 한계점이 존재한다.gunicorn은 WSGI이자 프로세스 관리자 역할을 수행한다. 따라서, gunicorn을 master 프로세스로 설정하고, uvicorn을 멀티 프로세스로 띄워 각각의 프로세스를 worker 프로세스로 설정할 수 있다.그러나, Kubernetes, Docker Swarm 등 여러 시스템의 분산 컨테이너를 관리하는 클러스터가 있는 경우 uvicorn 단일 프로세스로 실행하고, pod 혹은 container의 replication을 늘리는 것이 더 효율적일 수 있다.(FastAPI docs - Replication 참조)3.1 WSGI, ASGI 순서도ReferenceREST API 개발로 알아보는 WSGI, ASGIASGI 웹 프레임워크 FastAPI 를 시작하며FastAPI 톺아보기 - 부제: python 백엔드 봄은 온다" }, { "title": "[Jekyll] 블로그 포스팅하는 방법", "url": "/posts/blogging-first/", "categories": "Blogging, Toturial", "tags": "Blog, jekyll, GitHub, Git", "date": "2022-06-14 00:00:00 +0900", "snippet": "글로 나를 표현하는 일은 정말로 어렵다. 노트 필기는 나름 잘하는 편이다. 학교를 다닐 때나, 일을 하면서 배웠던 것들은 대체로 잘 정리를 했다. 그러나, 이것들을 하나의 이야기로 풀어나가는 과정은 정말이지 너무나도 어렵다. 어릴 때 책을 많이 읽지 않아서 일까 아니면 글 쓰는 것에는 별다른 재능이 없는 것 일까. 어찌됐든 지금부터라도 글 쓰는 연습을 하려고 한다. 처음부터 너무 잘 쓰려고 하기보다는 우선 글 쓰는 것에 초점을 맞춰서..!(이 글 쓰는 것 마저도 오래 걸렸다) 글 쓰는 연습도 하고, 내가 배운 것들을 공유하기 위해 첫 번째 포스팅은 Jekyll Chirpy 테마를 사용하여 블로그 만드는 글로써 시작하겠다!목차 Settings Chirpy theme install Chirpy run server Upload GitHub Pages 1. Settings Jekyll 사용을 위해서는 ruby 가 필수이다. 아래에 첨부된 링크를 따라서 필요한 로컬 환경을 설정하자 ruby 설치 링크 링크에서는 ruby-3.1.1 을 설치하라고 하는데, 현재 릴리즈 버전은 ruby-3.1.2 이므로 해당 내용을 변경해서 설치를 진행했다. ※ 환경이 바뀔 때 마다 개발 환경을 새로 설치하는 것이 귀찮다면 jekyll-docker 를 통해 바로 배포할 수도 있다. 2. Chirpy theme install 블로그를 만들기 위해 여러 테마를 찾아보았는데 Chirpy 테마에 있는 Categoires와 Archives 레이아웃이 보기에 가장 깔끔하고 좋았다. Chirpy 테마는 두 가지 방식으로 설치할 수 있다.2.1 chirpy-starter본 포스팅에서는 두 번째 방식으로 설치를 진행했다. chirpy-starter 를 이용해서 설치하는 방식은 아주 쉬우나, 참조한 블로그에 의하면 커스터마이징 할 때 계속 문제가 발생한다고 한다(사실 무슨 차이인지는 잘 모르겠다 😉)2.2 chirpy fork3. Chirpy run server3.1 Creating a New Site chirpy fork 를 통해서 새로운 레퍼지토리를 생성한다. 이 때, 레퍼지토리의 이름은 &lt;GH_USERNAME&gt;.github.io 로 생성할 것3.2 Clone your Repository$ git clone &lt;github URL&gt; fork를 통해 생성된 레퍼지토리에서 git clone을 받는다.3.3 Installing Dependencies#chirpy 초기화 작업$ tools/init.sh...[INFO] Initialization successful!$ bundle#bundle 명령어 실행 시 권한 관련 에러가 나는 경우$ bundle config set --local path 'vendor/bundle'$ bundle install chirpy 초기화 작업을 진행하면 아래와 같은 파일들이 삭제된다. .travis.yml _posts 폴더 하위의 파일들 docs 폴더들 3.4 Running Local#WSL 버전$ bundle exec jekyll s#Mac 버전$ jekyll server 서버가 정상적으로 동작하면 http://127.0.0.1:4000/ 로 서버가 실행되는 것을 확인할 수 있다.4. Upload GitHub Pages master 브랜치에서 변경 사항들을 적용하여 remote 서버로 push GitHub 서버에 파일들을 업로드 한 후, 해당 프로젝트 Settings &gt; Pages 클릭 Branch를 gh-pages로 변경하면 GitHub Page 설정 완료! main, gh-pages 브랜치를 제외하고 모두 삭제" } ]
